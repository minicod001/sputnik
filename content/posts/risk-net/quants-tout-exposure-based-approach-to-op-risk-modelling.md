---
title: 'Quants tout exposure-based approach to op risk modelling'
date: 2018-06-28T12:00:00+02:00
categories: ['risk-net']
tags: ['2018', '201806', 'risk management']
description: 'Ebor especially suited to modelling loss events such as legal claims, say proponents'
---

{{< quote 4682b4 >}}_Ebor especially suited to modelling loss events such as legal claims, say proponents_{{< /quote >}}

Operational risk modelling has long been viewed as something of an alchemic process, reliant to a greater or lesser degree on making sense of patterns in historical losses to predict future capital requirements. Now, a group of op risk experts is proposing an alternative quantification technique based instead on current exposures and event frequencies – an approach the experts say has longevity for banks, even after the current internal models regime is scrapped.

The approach, dubbed the [exposure-based operational risk](https://www.risk.net/journal-of-operational-risk/5603216/operational-risk-measurement-beyond-the-loss-distribution-approach-an-exposure-based-methodology) (Ebor) model, aims to produce better outcomes than those achieved by historical severity curves favoured by the current own-models approach for certain subcategories of op risk, such as such as litigation risks or rogue trading risks.

The experts contend in their paper that Ebor offers several advantages over the loss distribution approach (LDA) – under which historical loss distributions are [assumed](http://www.risk.net/risk-management/5712301/has-op-risk-capital-peaked-for-us-banks) to be the best predictor of future loss patterns, and which has come to [dominate](http://www.risk.net/comment/5362691/how-to-save-op-risk-modelling) op risk modelling approaches, despite a number of known limitations.

The use of LDA for certain risk types has been observed to undercapitalise known loss events before they occur, and overcapitalise for risk after the losses materialise, creating inappropriate capital estimates, says Michael Einemann, one of the paper’s authors. Conversely, where a bank often has in-depth knowledge about the underlying risk, such as the likelihood and amount of a payment, it makes sense to use that information to more accurately predict exposure, he says.

“Predictive factors for the operational risks are not captured in LDA, but could be assigned using a combination of statistical modelling and expert judgement, allowing for factor-based quantification of capital requirements,” says Einemann.

For other risk types that have more predictable characteristics – which they are expected to maintain for at least the next 12 months – it is problematic to rely almost exclusively on the historical loss experience, argues the paper, [Operational risk measurement beyond the loss distribution approach: an exposure-based methodology](https://www.risk.net/journal-of-operational-risk/5603216/operational-risk-measurement-beyond-the-loss-distribution-approach-an-exposure-based-methodology).

Most obviously, litigation events related to the sale of mortgage-backed securities emerged after the financial crisis as a [huge source](https://www.risk.net/topics/mortgage-backed-securities-mbs) of operational risk. A [report by the Boston Consulting Group](http://image-src.bcg.com/BCG_COM/BCG-Staying-the-Course-in-Banking-Mar-2017_tcm9-146794.pdf) shows that the 50 largest US and European banks paid cumulative financial penalties of about $320 billion from 2007 through to the end of 2016, largely as a result of crisis-era misdeeds.

Since its inception under Basel II, the current advanced measurement approach (AMA) to modelling op risk losses has routinely attracted criticism from banks. Former Standard Chartered chief executive [Peter Sands](http://www.risk.net/risk-management/operational-risk/4538321/scrap-absurd-op-risk-rwa-framework-says-sands) has argued banks should cease trying to model large, hard-to-predict, infrequently occurring losses such as outsize regulatory fines, and find a different way of accounting for them.

The authors argue the Ebor framework could be used to develop a model for pending litigations, where the event triggering the filing of the litigation has already happened and only the final outcome of the court case has to be modelled. Conceptually, this model could be extended to include potential future litigations, such as those based on credit properties of an underlying issuance portfolio.

The paper asserts that the Ebor model can even be used for situations with potentially very large – but not infinite – exposures.

In another example of its potential utility, for rogue trading, the model could take into account a specific group of traders, with a homogeneous probability of going rogue as frequency exposure. It could then model, for each rogue trading event, the severity based on the size of a hidden position as severity exposure, and time to detection or market movement as severity risk factors.

The approach has a shelf life beyond the death of the AMA, argue the authors, pointing out that the AMA’s replacement – the standardised measurement approach – has been shown to suffer from a number of [deficiencies](http://www.risk.net/risk/news/2479988/sma-s-data-shortfalls-make-op-risk-review-a-must) that make it unsuitable for a risk-sensitive quantification of operational risks – reinforcing the need for alternative modelling techniques, which banks have said will still be needed for [Pillar 2](http://www.risk.net/risk-management/5403881/op-risk-modelling-to-survive-move-to-sma) capital calculations and internal risk management purposes.

Other potential applications of the Ebor model, the authors suggest, include satisfying the European Union-wide stress tests undertaken by the European Banking Authority, and the US Federal Reserve’s [Comprehensive Capital Analysis and Review programme](https://www.risk.net/topics/comprehensive-capital-analysis-and-review-ccar).

One of the key benefits of the Ebor model, they argue, is its ability to determine risk contributions for individual loss events. The increased model granularity combined with forward-looking expert assessment leads to a more realistic dynamic of capital estimates. Individual events can be modelled in a more granular and comprehensive way than in LDA models, facilitating a better reflection of loss-generating mechanisms as well as risk mitigants.

In the hypothetical example of potential losses from litigation, the model behaviour is illustrated for five different phases of the litigation life cycle: initial filing; first internal risk assessment; refinement when more information becomes available; the establishment of a provision to final payment; and closure of the matter.

These characteristics would be treated differently under the LDA, where historical cases determine the [frequency and severity](http://www.risk.net/risk-management/5712301/has-op-risk-capital-peaked-for-us-banks)variables specified for litigation risk. In addition, the litigation would have been ignored until the fourth phase – provisioning for payment – under a traditional LDA, leading to undercapitalisation at the beginning of the litigation life cycle and overcapitalisation after the loss materialises.

Einemann says: “The application of Ebor to a portfolio of pending litigations is particularly well suited for an exposure-based approach, due to better usage of existing information and more plausible model behaviour over the litigation lifecycle.”

He adds that Ebor can facilitate communication among quants, risk managers and business experts, to ensure that discussions aim to identify and manage underlying risk drivers, instead of solely debating historical losses.

“In our experience, non-quant experts are more willing to share expertise and data from their day-to-day business as input into Ebor models than to accept statistical relationships under the LDA if they have difficulties in understanding the link to the actual risk exposure,” he says.

The authors also propose the integration of Ebor and LDA models into hybrid frameworks, facilitating the migration of operational risk subtypes from a classical to an exposure-based treatment.

The paper acknowledges that, in general, the development, calibration and validation of Ebor models represent a challenging task, since new types of data and a higher degree of expert involvement across an institution would be required to support them. But in return, they argue, Ebor models promise a transparent quantitative framework for combining forward-looking assessments of subject matter experts, historical loss experience and point-in-time data, such as current portfolios, instead of relying mainly on historical loss data.

