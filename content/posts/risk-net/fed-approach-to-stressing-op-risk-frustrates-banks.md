---
title: 'Fed’s approach to stressing op risk frustrates banks'
date: 2020-08-06T12:00:00+02:00
categories: ['risk-net']
tags: ['2020', '202008', 'regulation', 'FED', 'stress testing', 'operational risk']
description: 'Regulator’s stress test results overshoot banks’ numbers, threatening capital plans'
---

{{< quote 4682b4 >}}_Regulator’s stress test results overshoot banks’ numbers, threatening capital plans_{{< /quote >}}

When top US banks took part in the Federal Reserve’s revamped stress tests earlier this year, they submitted their own best guesses as to how they’d fare under the watchdog’s worst-case scenario. But the conclusions they arrived at regarding their overall projected losses were very different to those of their regulator – and some of them are placing the blame on how the Fed accounts for operational risk.

“The idea is that when you are looking at the numbers for the Fed scenario, generated by the Fed, there is a hybrid number that is macro and idiosyncratic, whereas the number submitted by banks for that exercise is ‘more macro’,” says a model risk expert at a non-systemic bank that participated in the 2020 Dodd-Frank Act Stress Tests (DFAST).

The 33 firms that participated in DFAST were projected by the Fed to take $144 billion of operational risk losses over the nine-quarter stress test horizon. This is equivalent to 12% of total ‘noninterest expense’ losses estimated by the agency using its pre-provision net revenue models.

In some cases, the gaps were material – large discrepancies between ‘noninterest expense’ figures produced by the banks and the Fed offer a clue as to why (see figure 1). For example, Morgan Stanley forecast $61.4 billion of ‘noninterest expenses’ under the Fed’s severely adverse scenario this year, compared with the agency’s $72.9 billion – a 19% undershoot. Morgan Stanley declined to ccommen

{{< embedded "de5863fe-ceed-441f-91a6-d13a1c1022e1" >}}

reported own-model estimates that were significantly lower than the Fed’s. Wells Fargo recorded a 15.5% undershoot – $114 billion versus $134.9 billion. JP Morgan’s was closer, but still almost 9% lower than the Fed’s at $149.9 billion.

However, the Fed does not break out op risk losses at a bank-by-bank level. Nor do the firms themselves in their own DFAST disclosures. This makes a side-by-side comparison of the Fed’s and banks’ op risk loss estimates difficult.

Discrepancies between bank and Fed stress test results are nothing new – top firms have [frequently pleaded](https://www.risk.net/risk-management/6553826/ccar-disclosure-sheds-new-light-on-modelling-default-losses) with the watchdog to let them peek at the models it uses for the annual stress-testing programme.

But with the advent of the [stress capital buffer (SCB) regime](https://www.risk.net/regulation/7501576/the-feds-stress-capital-buffer-relaxed-but-not-relaxing), whereby banks’ binding risk-based capital requirements are set according to their peak-to-trough performance through the tests, the inability to guess what loss figures the Fed will spit out has ramifications for capital planning.

“We don’t know exactly how the Fed makes their assumptions. It would be good for us to see, and challenge, and have a healthy dialogue around some of these assumptions – and make these models better and more realistic,” says the capital manager at a systemic US bank.

In simple terms, the Fed op risk projections are an average of two outputs: one from a linear regression model and the other from a historical simulation model. The former captures the sensitivity of op risk losses to the DFAST scenario’s macroeconomic shock; the latter the variation in op risk loss size across different op risk events.

What’s key is that the regression model projects aggregate op risk losses for the industry over the stress-test horizon and dollops these out among firms according to their size. Each individual firm’s allocation is therefore informed by the loss expectancy of the group as a whole.

In contrast, the historical simulation model does use each firm’s historical loss severity and frequency to inform the size of op risk losses. This means the Fed’s figures use a blend of macro and idiosyncratic data to size amounts for each firm, which experts say exaggerate banks’ likely losses under stress.

This is particularly vexing for firms that incurred [huge op risk losses](https://www.risk.net/risk-management/5712301/has-op-risk-capital-peaked-for-us-banks) in the past, but have transformed since.

“If the Fed is using some of these legal settlements we had back in the day as a source of ‘noninterest expense’, my argument would be we’re no longer in those lines of business. But I cannot have that dialogue, because I don’t know exactly what they’re assuming,” says the capital manager.

“We think firms who simplified their business and changed their business model are being disadvantaged, because the Fed assumptions keep the historical losses that may no longer be relevant,” he adds.

The Fed’s models, though, preclude erasure of these past nightmares. If a large loss occurred at an institution years ago, the regression model assumes another large loss could possibly occur in the future under a different guise. Removing large past losses from the dataset arbitrarily would skew the model unjustifiably.

In contrast, the banks themselves use a variety of techniques, but typically gauge their op risk losses using internal loss data and regression models based on the correlation of this data to macroeconomic variables.

In the past, when banks lowballed estimates of their projected losses compared to the Fed, this was more a nuisance than a pressing concern, as all that mattered was passing the tests overall. But now, the size of the loss a firm incurs under the Fed’s model matters for the calibration of its SCB.

DFAST op risk losses thereby inform part of each bank’s overall risk-based capital requirement. A bank that estimates a low amount of op risk losses may anticipate a smaller SCB than the one it eventually receives because of the Fed’s much larger projection, upending its capital planning.

“You can see how the Fed could potentially use the stress test to impose op risk charges on those banks that have a more significant trading operation, because they are the ones more likely to be subject to a type of [‘London Whale’](https://www.risk.net/topics/london-whale)-type risk,” says a US-based lawyer specialising in bank capital who used to be employed at the Fed.

They suggest the new SCB allows the Fed to smuggle in a Pillar 2-like op risk charge into the capital framework. As the SCB only applies under the [standardised approach](https://www.risk.net/risk-management/7566911/us-banks-face-capital-hit-from-resurgent-advanced-approaches), which does not include a specific op risk charge, the new buffer offers the only route to ensuring op risks are properly capitalised for until the fully loaded Basel III regime comes into force.

“They [banks] very much view op risk as the real wildcard in DFAST, because it’s a way the Fed can exact a Pillar 2 bespoke capital charge in a way that isn’t super transparent and can’t be easily challenged,” the lawyer adds.

